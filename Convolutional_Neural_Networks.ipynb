{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Convolutional Neural Networks",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vt2fPA4-vLs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "8adfa825-9ecb-4e25-b84c-1c95febd5e8e"
      },
      "source": [
        "import keras\n",
        "keras.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.5.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzJZJwplA09_"
      },
      "source": [
        "# CNN\n",
        "\n",
        "05/2020\n",
        "\n",
        "# Resumo\n",
        "\n",
        "Neste notebook discutiremos o que é uma Rede Neural Convolucional, construiremos uma baseados nos conceitos aprendidos, e a utilizaremos para resolver um problema de classificação binária.\n",
        "\n",
        "# Sumário\n",
        "\n",
        " <ol>\n",
        "<li>1.Introdução</li>\n",
        "<li>2.Detalhes da Arquitetura</li>\n",
        "<li>3.Problema: Classificação de Imagens</li>\n",
        "<li>4.Bibliografia</li>\n",
        "</ol>\n",
        "\n",
        "## 1.Introdução\n",
        "\n",
        "Apesar de todas as dificuldades provenientes de se treinar Redes Neurais com muitas camadas intermediárias, ao longo dos anos foram desenvolvidas algumas técnicas para que se pudesse treinar esse tipo de Rede. Na área de reconhecimento e processamento de imagens, o tipo de rede neural que tem dominado as recentes pesquisas é a Rede Neural Convolucional (ou CNN da sigla em inglês) cuja arquitetura é desenvolvida de modo que seja possível treinar as redes de maneira eficiente.\n",
        "\n",
        "## 2.Detalhes da Arquitetura\n",
        "\n",
        "\n",
        "### 2.1.Local Receptive Fields\n",
        "\n",
        "Diferente das Redes Neurais comuns, em que as camadas eram interpretadas como uma linha vertical de neurônios, nas Redes Neurais Convolucionais, as camadas de entrada devem ser interpretadas como um quadrado ou retângulo de neurônios como na imagem a seguir:\n",
        "\n",
        "\n",
        "![alt text](http://neuralnetworksanddeeplearning.com/images/tikz42.png)\n",
        "\n",
        "No problema do MNIST, por exemplo, todas as imagens estão em escala de cinza em dimensão 28x28, portanto a camada de entrada de uma CNN que resolva esse problema deve ser quadrada de dimensão também 28x28.\n",
        "\n",
        "Contudo, nem todo conjunto de imagens é tão uniforme, é comum que as imagens estejam em dimensões diferentes umas das outras. Por isso é uma prática comum re-escalar as imagens no pré-processamento de modo que todas fiquem com dimensões iguais às da camada de entrada da CNN.\n",
        "\n",
        "Diferente das camadas densas, onde todos os neurônios de uma camada são conectados com todos os neurônios da próxima camada, nas redes neurais convolucionais, cada neurônio da primeira camada intermediária conecta um número limitado de neurônios da camada de entrada. Para exemplificar será utilizada uma ilustração:\n",
        "\n",
        "![alt text](http://neuralnetworksanddeeplearning.com/images/tikz44.png)\n",
        "\n",
        "Podemos ver que um neurônio está conectado a um quadrado 5x5 de neurônios na entrada, essa pequena região que é apenas um pedaço da imagem original é chamado de *Campo Receptivo Local*. O neurônio da um peso para conexão que faz na entrada para aprendizagem, e ainda tem um bias. É possível interpretar isso como se o neurônio estivesse \"aprendendo a analisar apenas seu próprio Campo Receptivo\".\n",
        "\n",
        "Para completar a próxima camada da CNN é como se os Campos Receptivos Locais \"deslizassem\" pela imagem até completá-la, gerando assim um neurônio na primeira camada intermediária para cada Campo Receptivo, seguindo a próxima imagem:\n",
        "\n",
        "![alt text](http://neuralnetworksanddeeplearning.com/images/tikz45.png)\n",
        "\n",
        "Seguindo assim até que todos os Campos Receptivos da imagem sejam contemplados, podemos ver que para 28x28 neurônios de entrada teremos apenas 24x24 na próxima camada, isso se dá pelo fato de utilizarmos um tamanho para os Campos Receptivos de 5x5.\n",
        "\n",
        "### 2.2.Feature Maps\n",
        "\n",
        "Um detalhe crucial para a arquitetura de uma CNN é que, por mais que cada neurônio de uma camada intermediária esteja conectado a apenas um pequeno pedaço da imagem, todos eles compartilham os mesmos pesos e bias. Sendo assim cada um dos 24x24 neurônios do exemplo que estamos construindo treinarão exatamente os mesmos pesos e bias. A saída de um neurônio qualquer dessa camada pode ser escrita por:\n",
        "\n",
        "$$ \\sigma \\left(b + \\sum \\limits _{l = 0} ^{4} \\sum \\limits _{m = 0} ^{4} w_{l, m} a_{j+l,k+m} \\right) (1)$$ \n",
        "\n",
        "Aqui seguindo a linguagem do Nielsen<sup>[1]</sup>, $\\sigma$ é a função de ativação da rede neural, $b$ é o valor do bias e $w_{l, m}$ é conjunto 5x5 de pesos, todos compartilhados. Além disso, é utilizado $a_{x,y}$ para denotar a ativação do neurônio da posição $(x,y)$ da camada de entrada.\n",
        "\n",
        "Esse método de compartilhamento de pesos e bias leva a um atributo interessante, que é que todos os neurônios desta camada estão aprendendo a analisar o mesmo recurso (ou feature) apenas em locais diferentes da imagem.\n",
        "\n",
        "Por esse motivo, o mapeamento dos dados da entrada para a camada intermediária comummente recebe o nome de *Mapa de Recursos* (ou feature map). Como cada feature map só faz a detecção de um recurso da imagem, para fazer um reconhecimento eficiente de imagens, precisamos de mais de um feature map.\n",
        "\n",
        "\n",
        "![alt text](http://neuralnetworksanddeeplearning.com/images/tikz46.png)\n",
        "\n",
        "O exemplo acima tem 3 mapas de recursos, cada qual com 5x5 pesos compartilhados e 1 bias. Isso significa que essa Rede pode detectar 3 diferentes tipos de features, cada uma podendo ser identificada ao longo de toda imagem. Apesar da imagem mostrar apenas 3 features, frequentemente se utiliza muito mais feature maps em uma só camada da Rede Neural.\n",
        "\n",
        "### 2.3.Pooling Layers\n",
        "\n",
        "Outro elemento presente na arquitetura das Redes Neurais Convolucionais são  as chamadas *Camadas de Agrupamento* (ou Pooling Layer), essas camadas habitualmente estão presentes depois de uma camada de Mapa de Recursos, e seu papel é simplificar a informação presente na Camada Convolucional.\n",
        "\n",
        "Cada Camada de Agrupamento pega a saída de um determinado Feature Map e gera uma versão condensada do mesmo, como se sintetizasse suas informações em um Feature Map menor. Por exemplo, cada unidade da Pooling Layer pode sintetizar uma região de 2x2 ou 3x3 da camada anterior, transformando toda essa informação em apenas um neurônio.\n",
        "\n",
        "Existem várias maneiras de fazer essa síntese de informação, mas a maneira mais comum, que também sera utilizada neste notebook, é a chamada de Max-Pooling. Ela pega a região a ser condensada, e tem como saída o maior valor de ativação da mesma.\n",
        "\n",
        "Como existem vários Feature Maps em uma só CNN, se aplica uma Pooling Layer para cada Feature Map, como na imagem a seguir:\n",
        "\n",
        "![alt text](http://neuralnetworksanddeeplearning.com/images/tikz48.png)\n",
        "\n",
        "### 2.4.Arquitetura Final\n",
        "\n",
        "Agora que sabemos todos os artefatos de uma Rede Neural Convolucional, podemos juntar todas as partes e verificar como é a estrutura de uma CNN completa. Para isso podemos simplesmente ligar todas as saídas da Max Pooling Layer e conectar densamente à camada de saída da nossa Rede Neural da seguinte forma:\n",
        "\n",
        "![alt text](http://neuralnetworksanddeeplearning.com/images/tikz49.png)\n",
        "\n",
        "A camada de saída da Rede Neural é idêntica às Redes Neurais rasas, pode ter ativação do tipo SOFTMAX para classificação por exemplo, ou sigmóide caso o problema seja de classificação binária.\n",
        "\n",
        "É comum ainda fazer adições a essa estrutura que criamos, como por exemplo adicionar mais Camadas Convolucionais seguidas de Pooling Layers, ou adicionar uma Camada Densa de Neurônios (completamente conectada), antes da camada de saída. Não há regra para essas alterações e elas devem se adequar ao problema atacado.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXqq_CEOXBCk"
      },
      "source": [
        "## 3.Problema: Classificação de Imagens\n",
        "\n",
        "Para exemplificar a utilização das CNNs nesse notebook, resolveremos um problema que seria difícil de lidar de outra maneira. Esse problema é o de classificação de imagens de Cachorros e Gatos. O objetivo é aparentemente simples, devemos classificar imagens de cachorros e gatos, para isso utilizaremos o dataset \"Dogs_Vs_Cats\" presente no Kaggle, com mais de 25 mil imagens que são parecidas com as seguintes:\n",
        "\n",
        "![alt text](https://camo.githubusercontent.com/aa544d57e3ba791348677ce6b8287394c85f76ba/68747470733a2f2f73332e616d617a6f6e6177732e636f6d2f626f6f6b2e6b657261732e696f2f696d672f6368352f636174735f76735f646f67735f73616d706c65732e6a7067)\n",
        "\n",
        "\n",
        "Vamos treinar uma Rede Neural Convolucional para que ela seja capaz de, quando receber uma imagem, classificar se nela está presente um cachorro, ou um gato. Será utilizada apenas uma parte do dataset, com 20 mil imagens de treino e 2 mil para validação."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6NRS0P_Z9BU"
      },
      "source": [
        "O primeiro passo é fazer a instalação do kaggle:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsVhjkcqaJYA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebe62862-0234-422a-ed21-759941dcf5ba"
      },
      "source": [
        "!pip install kaggle"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.5.30)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIBPDdkuaKHp"
      },
      "source": [
        "Depois devemos fazer o upload do API Token (obtida no site do kaggle), para o computador remoto, para isso podemos utilizar esse código:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gColD0n1zsM5",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "40dfe794-15ed-4052-d887-9fa83b2f5f53"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f38e9f03-c746-4665-b4ca-aebd8dbacf73\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f38e9f03-c746-4665-b4ca-aebd8dbacf73\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "User uploaded file \"kaggle.json\" with length 72 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4DwTp1paog2"
      },
      "source": [
        "Então podemos finalmente baixar o dataset direto do Kaggle, e descomprimí-lo com:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIpJwBhjA30J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ed5a6b3-68b9-4bb3-bbc2-74b553acfc82"
      },
      "source": [
        "!mkdir $HOME/.kaggle\n",
        "!mv kaggle.json $HOME/.kaggle/kaggle.json\n",
        "!chmod 600 $HOME/.kaggle/kaggle.json\n",
        "\n",
        "\n",
        "! kaggle competitions download -c 'dogs-vs-cats'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n",
            "mv: cannot stat 'kaggle.json': No such file or directory\n",
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4)\n",
            "Downloading sampleSubmission.csv to /content\n",
            "  0% 0.00/86.8k [00:00<?, ?B/s]\n",
            "100% 86.8k/86.8k [00:00<00:00, 33.3MB/s]\n",
            "Downloading train.zip to /content\n",
            " 96% 520M/543M [00:03<00:00, 157MB/s]\n",
            "100% 543M/543M [00:03<00:00, 154MB/s]\n",
            "Downloading test1.zip to /content\n",
            " 94% 254M/271M [00:01<00:00, 237MB/s]\n",
            "100% 271M/271M [00:01<00:00, 232MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlfSdtiaeuSr"
      },
      "source": [
        "!unzip -qq /content/train.zip -d /datalab/ "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZdAficfcR8y"
      },
      "source": [
        "Agora, com todas as imagens no computador remoto, vamos separá-las em pastas de modo que exista uma pasta para treino, uma para validação e uma para as imagens de teste.\n",
        "\n",
        "Nós pegaremos as 10 mil primeiras imagens de cachorros e de gatos e moveremos para a pasta de treino, e colocaremos mil imagens de cada tipo nas pastas de validação e teste. Em cada uma dessas pastas, cachorros e gatos serão separados em pastas diferentes.\n",
        "\n",
        "\n",
        "Podemos fazer isso com o código a seguir:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkprCQO2fQPo"
      },
      "source": [
        "import os, shutil\n",
        "\n",
        "original_dataset_dir = '/datalab/train/'\n",
        "\n",
        "\n",
        "dir_base = '/content/processed_datalab'\n",
        "if not os.path.exists(dir_base):\n",
        "    os.mkdir(dir_base)\n",
        "\n",
        "dir_treino = os.path.join(dir_base, 'train')\n",
        "if not os.path.exists(dir_treino):\n",
        "    os.mkdir(dir_treino)\n",
        "dir_valid = os.path.join(dir_base, 'validation')\n",
        "if not os.path.exists(dir_valid):\n",
        "    os.mkdir(dir_valid)\n",
        "dir_teste = os.path.join(dir_base, 'test')\n",
        "if not os.path.exists(dir_teste):\n",
        "    os.mkdir(dir_teste)\n",
        "\n",
        "train_cats_dir = os.path.join(dir_treino, 'cats')\n",
        "if not os.path.exists(train_cats_dir):\n",
        "    os.mkdir(train_cats_dir)\n",
        "\n",
        "train_dogs_dir = os.path.join(dir_treino, 'dogs')\n",
        "if not os.path.exists(train_dogs_dir):\n",
        "    os.mkdir(train_dogs_dir)\n",
        "\n",
        "validation_cats_dir = os.path.join(dir_valid, 'cats')\n",
        "if not os.path.exists(validation_cats_dir):\n",
        "    os.mkdir(validation_cats_dir)\n",
        "\n",
        "validation_dogs_dir = os.path.join(dir_valid, 'dogs')\n",
        "if not os.path.exists(validation_dogs_dir):\n",
        "    os.mkdir(validation_dogs_dir)\n",
        "\n",
        "test_cats_dir = os.path.join(dir_teste, 'cats')\n",
        "if not os.path.exists(test_cats_dir):\n",
        "    os.mkdir(test_cats_dir)\n",
        "\n",
        "test_dogs_dir = os.path.join(dir_teste, 'dogs')\n",
        "if not os.path.exists(test_dogs_dir):\n",
        "    os.mkdir(test_dogs_dir)\n",
        "\n",
        "fnames = ['cat.{}.jpg'.format(i) for i in range(10000)]\n",
        "for fname in fnames:\n",
        "    src = os.path.join(original_dataset_dir, fname)\n",
        "    dst = os.path.join(train_cats_dir, fname)\n",
        "    shutil.copyfile(src, dst)\n",
        "\n",
        "fnames = ['cat.{}.jpg'.format(i) for i in range(10000, 11000)]\n",
        "for fname in fnames:\n",
        "    src = os.path.join(original_dataset_dir, fname)\n",
        "    dst = os.path.join(validation_cats_dir, fname)\n",
        "    shutil.copyfile(src, dst)\n",
        "    \n",
        "fnames = ['cat.{}.jpg'.format(i) for i in range(11000, 12000)]\n",
        "for fname in fnames:\n",
        "    src = os.path.join(original_dataset_dir, fname)\n",
        "    dst = os.path.join(test_cats_dir, fname)\n",
        "    shutil.copyfile(src, dst)\n",
        "    \n",
        "fnames = ['dog.{}.jpg'.format(i) for i in range(10000)]\n",
        "for fname in fnames:\n",
        "    src = os.path.join(original_dataset_dir, fname)\n",
        "    dst = os.path.join(train_dogs_dir, fname)\n",
        "    shutil.copyfile(src, dst)\n",
        "    \n",
        "fnames = ['dog.{}.jpg'.format(i) for i in range(10000, 11000)]\n",
        "for fname in fnames:\n",
        "    src = os.path.join(original_dataset_dir, fname)\n",
        "    dst = os.path.join(validation_dogs_dir, fname)\n",
        "    shutil.copyfile(src, dst)\n",
        "    \n",
        "fnames = ['dog.{}.jpg'.format(i) for i in range(11000, 12000)]\n",
        "for fname in fnames:\n",
        "    src = os.path.join(original_dataset_dir, fname)\n",
        "    dst = os.path.join(test_dogs_dir, fname)\n",
        "    shutil.copyfile(src, dst)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjZkggdqeZ4l"
      },
      "source": [
        "Mesmo com as imagens devidamente separadas, elas ainda não estão prontas para entrar em uma CNN. É necessário que se faça um pré-processamento das imagens para que todas fiquem na mesma escala e dimensão para que possam ser reconhecidas pela camada de entrada da nossa Rede Neural Convolucional. Mudaremos todas as imagens para escala de cinza de 8 bits, e deixaremos todas com medida 150x150.\n",
        "\n",
        "Utilizaremos o *ImageDataGenerator* do Keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MESbKg9ifUW_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4779af1f-a183-4d62-cb6b-8dc0aae9c92b"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        dir_treino,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=20,\n",
        "        class_mode='binary')\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "        dir_valid,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=20,\n",
        "        class_mode='binary')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 20000 images belonging to 2 classes.\n",
            "Found 2000 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vN8kmwiag8Me"
      },
      "source": [
        "Com nossas imagens prontas, vamos desenvolver agora a nossa Rede Neural.\n",
        "\n",
        "Vamos adicionar 4 conjuntos Convolutional Layer e Max Pooling, com entrada 150x150, todas com Campos Receptivos Locais 5x5 e Região de Agrupamento 2x2. O número de feature maps irá aumentando ao longo das camadas, começando com 20, passando por 40 e tendo duas Camadas Convolucionais de 80 feature maps. A função de ativação utilizada será o Retificador Linear (ReLU).\n",
        "\n",
        "Podemos criar essa Rede Neural com o seguinte código:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWSMYcpZgR2J"
      },
      "source": [
        "from keras import layers\n",
        "from keras import models\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(16, (3, 3), activation='relu',\n",
        "                        input_shape=(150, 150, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2hRKdmojoKT"
      },
      "source": [
        "Como as camadas até aqui tem duas dimensões, e as próximas camadas utilizadas tem apenas uma dimensão, precisaremos \"achatar\" os dados, de modo que eles fiquem com apenas uma dimensão, podemos fazer isso dessa maneira:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-9lCju8j0gr"
      },
      "source": [
        "model.add(layers.Flatten())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ef6zP9gkLlw"
      },
      "source": [
        "Agora adicionaremos uma camada com regularização do tipo dropout, para diminuir o efeito do overfitting, e adicionaremos uma camada densa de 100 neurônios, além da saída com a função de ativação sigmóide."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDRrF-VFiITt"
      },
      "source": [
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(512, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vxKyIG3kdB3"
      },
      "source": [
        "Os hiper-parâmetros escolhidos neste notebook foram escolhidos baseados nos notebooks do Chollet<sup>[2]</sup>, no livro do Nielsen<sup>[1]</sup> e em testes empíricos.\n",
        "\n",
        "Com a função *summary* podemos ver como ficou nossa rede neural:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hD2EAAEbgUPS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9055193e-de02-4099-b9bd-13ff2c345a9c"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 148, 148, 16)      448       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 74, 74, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 72, 72, 32)        4640      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 36, 36, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 34, 34, 32)        9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 17, 17, 32)        0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 9248)              0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 9248)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 512)               4735488   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 513       \n",
            "=================================================================\n",
            "Total params: 4,750,337\n",
            "Trainable params: 4,750,337\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAr5etXklGuC"
      },
      "source": [
        "Agora precisamos escolher nossos otimizadores:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiBDuR9EVxbF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "689a2bf3-e8d2-4a74-b376-ed6c598f466a"
      },
      "source": [
        "from keras import optimizers\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=optimizers.RMSprop(lr=1e-4),\n",
        "              metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSbJN0JwlTH6"
      },
      "source": [
        "E enfim, podemos fazer o treino da nossa CNN, como os dados foram gerados pelo *DataGenerator*, nós teremos que utilizar a função fit_generator. Treinaremos para 50 épocas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jSkqnfqXAYI",
        "outputId": "8c2d626e-8249-40a5-d4cb-9a66ab63e51a"
      },
      "source": [
        "print(train_generator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<keras.preprocessing.image.DirectoryIterator object at 0x7f74a2457510>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xX-2OOsV2Xp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6719532c-f9e7-4297-fa83-23bbe179cf27"
      },
      "source": [
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch=100,\n",
        "      epochs=100,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps=50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:1915: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "100/100 [==============================] - 53s 96ms/step - loss: 0.7256 - acc: 0.5049 - val_loss: 0.6726 - val_acc: 0.6330\n",
            "Epoch 2/100\n",
            "100/100 [==============================] - 9s 86ms/step - loss: 0.6813 - acc: 0.5718 - val_loss: 0.6746 - val_acc: 0.5510\n",
            "Epoch 3/100\n",
            "100/100 [==============================] - 9s 91ms/step - loss: 0.6665 - acc: 0.5943 - val_loss: 0.6412 - val_acc: 0.6490\n",
            "Epoch 4/100\n",
            "100/100 [==============================] - 9s 91ms/step - loss: 0.6548 - acc: 0.6003 - val_loss: 0.6247 - val_acc: 0.6500\n",
            "Epoch 5/100\n",
            "100/100 [==============================] - 9s 88ms/step - loss: 0.6325 - acc: 0.6293 - val_loss: 0.6659 - val_acc: 0.5810\n",
            "Epoch 6/100\n",
            "100/100 [==============================] - 8s 84ms/step - loss: 0.6073 - acc: 0.6689 - val_loss: 0.6142 - val_acc: 0.6680\n",
            "Epoch 7/100\n",
            "100/100 [==============================] - 8s 85ms/step - loss: 0.5998 - acc: 0.6891 - val_loss: 0.5878 - val_acc: 0.6870\n",
            "Epoch 8/100\n",
            "100/100 [==============================] - 8s 84ms/step - loss: 0.5880 - acc: 0.6842 - val_loss: 0.5750 - val_acc: 0.7010\n",
            "Epoch 9/100\n",
            "100/100 [==============================] - 9s 86ms/step - loss: 0.5875 - acc: 0.6838 - val_loss: 0.5631 - val_acc: 0.7030\n",
            "Epoch 10/100\n",
            "100/100 [==============================] - 9s 92ms/step - loss: 0.5891 - acc: 0.6993 - val_loss: 0.5509 - val_acc: 0.7320\n",
            "Epoch 11/100\n",
            "100/100 [==============================] - 9s 91ms/step - loss: 0.5600 - acc: 0.6972 - val_loss: 0.5315 - val_acc: 0.7380\n",
            "Epoch 12/100\n",
            "100/100 [==============================] - 9s 91ms/step - loss: 0.5583 - acc: 0.7065 - val_loss: 0.5540 - val_acc: 0.7120\n",
            "Epoch 13/100\n",
            "100/100 [==============================] - 8s 84ms/step - loss: 0.5690 - acc: 0.7053 - val_loss: 0.5360 - val_acc: 0.7170\n",
            "Epoch 14/100\n",
            "100/100 [==============================] - 8s 85ms/step - loss: 0.5271 - acc: 0.7196 - val_loss: 0.5784 - val_acc: 0.7000\n",
            "Epoch 15/100\n",
            "100/100 [==============================] - 8s 84ms/step - loss: 0.5463 - acc: 0.7216 - val_loss: 0.5386 - val_acc: 0.7230\n",
            "Epoch 16/100\n",
            "100/100 [==============================] - 8s 85ms/step - loss: 0.5307 - acc: 0.7241 - val_loss: 0.5538 - val_acc: 0.7080\n",
            "Epoch 17/100\n",
            "100/100 [==============================] - 9s 90ms/step - loss: 0.5199 - acc: 0.7493 - val_loss: 0.5346 - val_acc: 0.7170\n",
            "Epoch 18/100\n",
            "100/100 [==============================] - 9s 90ms/step - loss: 0.5386 - acc: 0.7319 - val_loss: 0.4969 - val_acc: 0.7560\n",
            "Epoch 19/100\n",
            "100/100 [==============================] - 9s 89ms/step - loss: 0.4961 - acc: 0.7619 - val_loss: 0.5220 - val_acc: 0.7400\n",
            "Epoch 20/100\n",
            "100/100 [==============================] - 9s 86ms/step - loss: 0.5088 - acc: 0.7488 - val_loss: 0.5729 - val_acc: 0.7080\n",
            "Epoch 21/100\n",
            "100/100 [==============================] - 9s 86ms/step - loss: 0.5013 - acc: 0.7579 - val_loss: 0.5327 - val_acc: 0.7390\n",
            "Epoch 22/100\n",
            "100/100 [==============================] - 9s 86ms/step - loss: 0.5161 - acc: 0.7493 - val_loss: 0.4866 - val_acc: 0.7550\n",
            "Epoch 23/100\n",
            "100/100 [==============================] - 9s 86ms/step - loss: 0.5283 - acc: 0.7488 - val_loss: 0.4851 - val_acc: 0.7610\n",
            "Epoch 24/100\n",
            "100/100 [==============================] - 9s 91ms/step - loss: 0.5082 - acc: 0.7571 - val_loss: 0.5107 - val_acc: 0.7530\n",
            "Epoch 25/100\n",
            "100/100 [==============================] - 9s 92ms/step - loss: 0.4866 - acc: 0.7559 - val_loss: 0.5285 - val_acc: 0.7370\n",
            "Epoch 26/100\n",
            "100/100 [==============================] - 9s 87ms/step - loss: 0.4947 - acc: 0.7466 - val_loss: 0.5061 - val_acc: 0.7460\n",
            "Epoch 27/100\n",
            "100/100 [==============================] - 9s 86ms/step - loss: 0.5122 - acc: 0.7453 - val_loss: 0.4718 - val_acc: 0.7750\n",
            "Epoch 28/100\n",
            "100/100 [==============================] - 8s 85ms/step - loss: 0.4719 - acc: 0.7735 - val_loss: 0.4658 - val_acc: 0.7710\n",
            "Epoch 29/100\n",
            "100/100 [==============================] - 9s 85ms/step - loss: 0.4704 - acc: 0.7748 - val_loss: 0.4930 - val_acc: 0.7400\n",
            "Epoch 30/100\n",
            "100/100 [==============================] - 9s 87ms/step - loss: 0.4794 - acc: 0.7757 - val_loss: 0.4729 - val_acc: 0.7750\n",
            "Epoch 31/100\n",
            "100/100 [==============================] - 9s 92ms/step - loss: 0.4881 - acc: 0.7673 - val_loss: 0.4993 - val_acc: 0.7520\n",
            "Epoch 32/100\n",
            "100/100 [==============================] - 9s 90ms/step - loss: 0.4547 - acc: 0.7922 - val_loss: 0.4863 - val_acc: 0.7650\n",
            "Epoch 33/100\n",
            "100/100 [==============================] - 9s 87ms/step - loss: 0.4945 - acc: 0.7689 - val_loss: 0.5651 - val_acc: 0.7260\n",
            "Epoch 34/100\n",
            "100/100 [==============================] - 8s 85ms/step - loss: 0.4866 - acc: 0.7642 - val_loss: 0.4963 - val_acc: 0.7470\n",
            "Epoch 35/100\n",
            "100/100 [==============================] - 9s 87ms/step - loss: 0.4729 - acc: 0.7744 - val_loss: 0.4676 - val_acc: 0.7830\n",
            "Epoch 36/100\n",
            "100/100 [==============================] - 9s 86ms/step - loss: 0.4875 - acc: 0.7615 - val_loss: 0.4774 - val_acc: 0.7820\n",
            "Epoch 37/100\n",
            "100/100 [==============================] - 9s 89ms/step - loss: 0.4626 - acc: 0.7831 - val_loss: 0.4772 - val_acc: 0.7590\n",
            "Epoch 38/100\n",
            "100/100 [==============================] - 9s 90ms/step - loss: 0.4814 - acc: 0.7721 - val_loss: 0.4811 - val_acc: 0.7550\n",
            "Epoch 39/100\n",
            "100/100 [==============================] - 9s 91ms/step - loss: 0.4724 - acc: 0.7758 - val_loss: 0.4757 - val_acc: 0.7700\n",
            "Epoch 40/100\n",
            "100/100 [==============================] - 8s 85ms/step - loss: 0.4760 - acc: 0.7830 - val_loss: 0.5002 - val_acc: 0.7610\n",
            "Epoch 41/100\n",
            "100/100 [==============================] - 9s 90ms/step - loss: 0.4535 - acc: 0.7851 - val_loss: 0.4581 - val_acc: 0.7770\n",
            "Epoch 42/100\n",
            "100/100 [==============================] - 9s 91ms/step - loss: 0.4474 - acc: 0.7852 - val_loss: 0.4429 - val_acc: 0.7920\n",
            "Epoch 43/100\n",
            "100/100 [==============================] - 9s 89ms/step - loss: 0.4546 - acc: 0.8108 - val_loss: 0.4555 - val_acc: 0.7860\n",
            "Epoch 44/100\n",
            "100/100 [==============================] - 9s 86ms/step - loss: 0.4391 - acc: 0.7934 - val_loss: 0.4324 - val_acc: 0.7900\n",
            "Epoch 45/100\n",
            "100/100 [==============================] - 8s 85ms/step - loss: 0.4477 - acc: 0.7983 - val_loss: 0.4437 - val_acc: 0.7820\n",
            "Epoch 46/100\n",
            "100/100 [==============================] - 8s 85ms/step - loss: 0.4404 - acc: 0.7918 - val_loss: 0.4683 - val_acc: 0.7730\n",
            "Epoch 47/100\n",
            "100/100 [==============================] - 9s 87ms/step - loss: 0.4268 - acc: 0.7958 - val_loss: 0.4390 - val_acc: 0.7980\n",
            "Epoch 48/100\n",
            "100/100 [==============================] - 9s 91ms/step - loss: 0.4778 - acc: 0.7576 - val_loss: 0.4617 - val_acc: 0.7650\n",
            "Epoch 49/100\n",
            "100/100 [==============================] - 9s 91ms/step - loss: 0.4458 - acc: 0.7937 - val_loss: 0.5353 - val_acc: 0.7420\n",
            "Epoch 50/100\n",
            "100/100 [==============================] - 8s 84ms/step - loss: 0.4586 - acc: 0.7772 - val_loss: 0.4516 - val_acc: 0.7810\n",
            "Epoch 51/100\n",
            "100/100 [==============================] - 8s 85ms/step - loss: 0.4447 - acc: 0.7861 - val_loss: 0.4508 - val_acc: 0.7650\n",
            "Epoch 52/100\n",
            "100/100 [==============================] - 9s 85ms/step - loss: 0.4266 - acc: 0.8004 - val_loss: 0.4496 - val_acc: 0.7850\n",
            "Epoch 53/100\n",
            "100/100 [==============================] - 9s 86ms/step - loss: 0.4231 - acc: 0.8137 - val_loss: 0.4689 - val_acc: 0.7820\n",
            "Epoch 54/100\n",
            "100/100 [==============================] - 9s 90ms/step - loss: 0.4218 - acc: 0.8060 - val_loss: 0.4442 - val_acc: 0.7910\n",
            "Epoch 55/100\n",
            "100/100 [==============================] - 9s 91ms/step - loss: 0.4229 - acc: 0.8006 - val_loss: 0.4679 - val_acc: 0.7800\n",
            "Epoch 56/100\n",
            "100/100 [==============================] - 9s 89ms/step - loss: 0.4247 - acc: 0.8070 - val_loss: 0.4377 - val_acc: 0.7810\n",
            "Epoch 57/100\n",
            "100/100 [==============================] - 8s 84ms/step - loss: 0.4207 - acc: 0.8006 - val_loss: 0.4791 - val_acc: 0.7690\n",
            "Epoch 58/100\n",
            "100/100 [==============================] - 8s 85ms/step - loss: 0.4309 - acc: 0.8035 - val_loss: 0.4344 - val_acc: 0.7960\n",
            "Epoch 59/100\n",
            "100/100 [==============================] - 8s 85ms/step - loss: 0.4349 - acc: 0.8119 - val_loss: 0.4721 - val_acc: 0.7690\n",
            "Epoch 60/100\n",
            "100/100 [==============================] - 9s 86ms/step - loss: 0.4329 - acc: 0.7991 - val_loss: 0.4879 - val_acc: 0.7610\n",
            "Epoch 61/100\n",
            "100/100 [==============================] - 9s 91ms/step - loss: 0.4433 - acc: 0.7839 - val_loss: 0.4473 - val_acc: 0.7880\n",
            "Epoch 62/100\n",
            "100/100 [==============================] - 9s 91ms/step - loss: 0.4380 - acc: 0.8015 - val_loss: 0.4472 - val_acc: 0.7710\n",
            "Epoch 63/100\n",
            "100/100 [==============================] - 9s 90ms/step - loss: 0.4239 - acc: 0.8068 - val_loss: 0.4508 - val_acc: 0.7810\n",
            "Epoch 64/100\n",
            "100/100 [==============================] - 8s 84ms/step - loss: 0.4033 - acc: 0.8235 - val_loss: 0.4367 - val_acc: 0.7900\n",
            "Epoch 65/100\n",
            "100/100 [==============================] - 8s 85ms/step - loss: 0.4281 - acc: 0.7843 - val_loss: 0.4272 - val_acc: 0.7860\n",
            "Epoch 66/100\n",
            "100/100 [==============================] - 8s 83ms/step - loss: 0.4168 - acc: 0.8133 - val_loss: 0.4635 - val_acc: 0.7690\n",
            "Epoch 67/100\n",
            "100/100 [==============================] - 9s 85ms/step - loss: 0.4164 - acc: 0.8075 - val_loss: 0.4290 - val_acc: 0.8070\n",
            "Epoch 68/100\n",
            "100/100 [==============================] - 9s 92ms/step - loss: 0.4098 - acc: 0.8244 - val_loss: 0.4684 - val_acc: 0.7700\n",
            "Epoch 69/100\n",
            "100/100 [==============================] - 9s 91ms/step - loss: 0.4240 - acc: 0.8102 - val_loss: 0.4571 - val_acc: 0.7760\n",
            "Epoch 70/100\n",
            "100/100 [==============================] - 9s 89ms/step - loss: 0.4277 - acc: 0.8021 - val_loss: 0.4344 - val_acc: 0.7980\n",
            "Epoch 71/100\n",
            "100/100 [==============================] - 9s 87ms/step - loss: 0.4258 - acc: 0.8055 - val_loss: 0.4193 - val_acc: 0.8080\n",
            "Epoch 72/100\n",
            "100/100 [==============================] - 9s 90ms/step - loss: 0.4019 - acc: 0.8113 - val_loss: 0.4489 - val_acc: 0.7800\n",
            "Epoch 73/100\n",
            "100/100 [==============================] - 9s 91ms/step - loss: 0.4058 - acc: 0.8052 - val_loss: 0.4417 - val_acc: 0.7900\n",
            "Epoch 74/100\n",
            "100/100 [==============================] - 9s 86ms/step - loss: 0.4079 - acc: 0.8032 - val_loss: 0.4340 - val_acc: 0.7960\n",
            "Epoch 75/100\n",
            "100/100 [==============================] - 8s 85ms/step - loss: 0.4264 - acc: 0.8024 - val_loss: 0.4273 - val_acc: 0.8040\n",
            "Epoch 76/100\n",
            "100/100 [==============================] - 9s 86ms/step - loss: 0.4265 - acc: 0.8092 - val_loss: 0.4367 - val_acc: 0.8000\n",
            "Epoch 77/100\n",
            "100/100 [==============================] - 8s 85ms/step - loss: 0.3942 - acc: 0.8161 - val_loss: 0.4764 - val_acc: 0.7640\n",
            "Epoch 78/100\n",
            "100/100 [==============================] - 9s 89ms/step - loss: 0.4268 - acc: 0.8027 - val_loss: 0.4270 - val_acc: 0.8050\n",
            "Epoch 79/100\n",
            "100/100 [==============================] - 9s 91ms/step - loss: 0.4292 - acc: 0.8110 - val_loss: 0.4120 - val_acc: 0.7990\n",
            "Epoch 80/100\n",
            "100/100 [==============================] - 9s 90ms/step - loss: 0.3656 - acc: 0.8453 - val_loss: 0.4279 - val_acc: 0.7870\n",
            "Epoch 81/100\n",
            "100/100 [==============================] - 8s 84ms/step - loss: 0.4044 - acc: 0.8350 - val_loss: 0.4259 - val_acc: 0.8080\n",
            "Epoch 82/100\n",
            "100/100 [==============================] - 9s 85ms/step - loss: 0.4189 - acc: 0.8134 - val_loss: 0.4315 - val_acc: 0.7890\n",
            "Epoch 83/100\n",
            "100/100 [==============================] - 8s 85ms/step - loss: 0.3767 - acc: 0.8339 - val_loss: 0.4230 - val_acc: 0.7970\n",
            "Epoch 84/100\n",
            "100/100 [==============================] - 8s 84ms/step - loss: 0.4286 - acc: 0.8021 - val_loss: 0.4301 - val_acc: 0.7980\n",
            "Epoch 85/100\n",
            "100/100 [==============================] - 9s 88ms/step - loss: 0.3853 - acc: 0.8281 - val_loss: 0.4194 - val_acc: 0.8060\n",
            "Epoch 86/100\n",
            "100/100 [==============================] - 9s 91ms/step - loss: 0.4075 - acc: 0.8133 - val_loss: 0.4280 - val_acc: 0.7930\n",
            "Epoch 87/100\n",
            "100/100 [==============================] - 9s 92ms/step - loss: 0.3895 - acc: 0.8271 - val_loss: 0.4154 - val_acc: 0.8090\n",
            "Epoch 88/100\n",
            "100/100 [==============================] - 9s 86ms/step - loss: 0.3742 - acc: 0.8369 - val_loss: 0.4755 - val_acc: 0.7750\n",
            "Epoch 89/100\n",
            "100/100 [==============================] - 9s 85ms/step - loss: 0.4165 - acc: 0.8058 - val_loss: 0.4242 - val_acc: 0.7890\n",
            "Epoch 90/100\n",
            "100/100 [==============================] - 9s 85ms/step - loss: 0.4006 - acc: 0.8170 - val_loss: 0.4290 - val_acc: 0.7920\n",
            "Epoch 91/100\n",
            "100/100 [==============================] - 8s 83ms/step - loss: 0.4037 - acc: 0.8149 - val_loss: 0.4151 - val_acc: 0.8060\n",
            "Epoch 92/100\n",
            "100/100 [==============================] - 9s 88ms/step - loss: 0.3896 - acc: 0.8287 - val_loss: 0.4287 - val_acc: 0.7990\n",
            "Epoch 93/100\n",
            "100/100 [==============================] - 9s 91ms/step - loss: 0.4029 - acc: 0.8141 - val_loss: 0.4199 - val_acc: 0.8040\n",
            "Epoch 94/100\n",
            "100/100 [==============================] - 9s 89ms/step - loss: 0.3721 - acc: 0.8383 - val_loss: 0.4125 - val_acc: 0.8040\n",
            "Epoch 95/100\n",
            "100/100 [==============================] - 9s 86ms/step - loss: 0.4103 - acc: 0.8246 - val_loss: 0.4402 - val_acc: 0.7870\n",
            "Epoch 96/100\n",
            "100/100 [==============================] - 8s 84ms/step - loss: 0.3968 - acc: 0.8286 - val_loss: 0.4209 - val_acc: 0.8050\n",
            "Epoch 97/100\n",
            "100/100 [==============================] - 8s 84ms/step - loss: 0.3594 - acc: 0.8361 - val_loss: 0.4441 - val_acc: 0.8000\n",
            "Epoch 98/100\n",
            "100/100 [==============================] - 8s 85ms/step - loss: 0.3716 - acc: 0.8314 - val_loss: 0.4229 - val_acc: 0.8040\n",
            "Epoch 99/100\n",
            "100/100 [==============================] - 9s 88ms/step - loss: 0.3993 - acc: 0.8210 - val_loss: 0.4270 - val_acc: 0.8010\n",
            "Epoch 100/100\n",
            "100/100 [==============================] - 9s 91ms/step - loss: 0.3921 - acc: 0.8282 - val_loss: 0.4199 - val_acc: 0.8030\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LG1KivNlaGP2"
      },
      "source": [
        "model.save('cats_and_dogs_test.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzC2FqYMnNqr"
      },
      "source": [
        "Agora podemos avaliar os resultados:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGB3Uid_lQ89"
      },
      "source": [
        "model.save('cats_and_dogs_small_2.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nl9KexkFV6XX"
      },
      "source": [
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21WdqZeXXRd0"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_q6LwxmuUjq"
      },
      "source": [
        "model.save('cats_and_dogs_small_2.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCDsXvMEfPVr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4a82d846-eac7-422f-b91c-cefd527d28db"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "np.shape(train_generator[0][0][0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(150, 150, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZZOz5eNnoHx"
      },
      "source": [
        "A instabilidade da função de custo mostra que ainda há espaço para melhoria, apesar disso, a CNN criada do zero conseguiu atingir ótimos resultados, uma acurácia de cerca de 87%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCcN08zdpa50"
      },
      "source": [
        "##5. Bibliografia \n",
        "\n",
        "[1] http://neuralnetworksanddeeplearning.com/index.html\n",
        "\n",
        "[2] https://github.com/fchollet/deep-learning-with-python-notebooks\n",
        "\n",
        "[3] https://colab.research.google.com/drive/19SVdlmnn6yRXCvNnE8PT1vbXrA8FrBo_#scrollTo=1HrGpk_b4dvL\n",
        "\n",
        "[4] https://www.kaggle.com/c/dogs-vs-cats\n",
        "\n",
        "[5] https://machinelearningmastery.com/how-to-develop-a-convolutional-neural-network-to-classify-photos-of-dogs-and-cats/\n"
      ]
    }
  ]
}